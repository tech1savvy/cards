---
id: 12.absence-of-knowledge
aliases: []
tags: []
---

**Escape Valve**: _Predefine fallback actions for when the GPT cannot answer clearly_, such as:

- Refusing to answer and directing to support staff.
- Suggesting alternative policy-approved options.
- Providing a creative but bounded workaround.

---

**Knowledge Ambiguity**: Policies and documents often don’t explicitly cover every case, creating uncertainty when answering.
**Multiple Sources of Ambiguity**:

- Policy gaps (e.g., no mention of Southwest Business Select).
- User prompts about things not directly mapped or defined.
  **GPT Tendency to Over-Answer**: By default, GPTs try to fill gaps and provide an answer, which risks hallucination if information is incomplete.
  **Explicitly Define Ambiguity**: Clearly state what counts as ambiguity (e.g., requiring an inference without a direct statement, or missing quotations from policies).
  **Instructions for Ambiguity**: Include rules such as:
- If there’s no direct policy statement → don’t answer.
- If no quotations exist in the knowledge base → refer to help.
- Or provide acceptable alternatives (similar options explicitly in policy).
  **Avoiding False Confidence**: Without clear rules, the GPT risks telling users inconsistent or inaccurate information depending on how prompts are phrased.
  **Consistency Across Instances**: Two GPTs using the same knowledge base can still diverge if ambiguity isn’t controlled—boundaries prevent this.
  **User Awareness**: Ensure humans know when the knowledge base lacks clarity so they don’t take the AI’s invented answer as fact.
  **Design Principle**: Always define what to do “when there isn’t a clear-cut answer” so the GPT has a safe behavior instead of improvising.

Would you like me to create a **decision-flow template** (evidence available → ambiguity detected → fallback path) that you can embed in your custom GPT instructions to enforce this handling?

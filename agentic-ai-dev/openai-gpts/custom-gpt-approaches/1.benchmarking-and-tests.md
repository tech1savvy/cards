---
id: benchmarking-and-tests
aliases: []
tags: []
---

## Benchmarking

> A benchmark provides a way to test whether the custom GPT continues to perform well after modifications.
> Helps detect **regressions** (where _performance gets worse on tasks it previously handled well_).
> Confirms that the GPT performs as well as expected across tasks.

### Benchmark structure

- **Test cases**: A table of user prompts with expected answers.
- **Expected answers**: May be exact (right/wrong) or qualitative examples of good output.
- **Rubric**: Clear _criteria_ describing _what makes an output good or bad_.
- **Scoring scale**: Typically 1–10 (or 0–100) to measure performance consistently.

---

## Structure of a Test Case

Each test case should include:

- **Title** – a short, descriptive heading.
- **Goal** – what is being tested and why.
- **User Prompt** – example input from a user.
- **Correct Answer (to be filled in)** – baseline expected response from the GPT, possibly provided from a knowledge base (e.g., Vanderbilt’s travel policy).
- **Rubric** – criteria for grading the GPT’s response, considering clarity, compliance, completeness, etc.
